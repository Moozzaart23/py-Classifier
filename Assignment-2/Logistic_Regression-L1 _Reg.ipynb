{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadPreprocessData:\n",
    "    def __init__(self, path, header = None):\n",
    "        self.dataset = pd.read_csv(path, header = header)\n",
    "        self.X = self.dataset.iloc[:, :4]\n",
    "        self.y = self.dataset.iloc[:, 4]\n",
    "        \n",
    "    def normalize(self, axis = 0):\n",
    "        return ((self.X - self.X.mean(axis = axis)) / self.X.std(axis = axis))\n",
    "    \n",
    "    def train_test_split(self, percent = 80):\n",
    "        train_rows = random.sample(range(0, self.y.size), percent * self.y.size // 100)\n",
    "        train_rows.sort()\n",
    "        test_rows=[rows for rows in self.X.index.values if rows not in train_rows]\n",
    "        self.train_X = self.X.iloc[train_rows].reset_index(drop = True) \n",
    "        self.train_y = self.y.iloc[train_rows].reset_index(drop = True) \n",
    "        \n",
    "        self.test_X = self.X.iloc[test_rows].reset_index(drop = True) \n",
    "        self.test_y = self.y.iloc[test_rows].reset_index(drop = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.36840</td>\n",
       "      <td>9.67180</td>\n",
       "      <td>-3.9606</td>\n",
       "      <td>-3.16250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1        2        3\n",
       "0     3.62160   8.66610  -2.8073 -0.44699\n",
       "1     4.54590   8.16740  -2.4586 -1.46210\n",
       "2     3.86600  -2.63830   1.9242  0.10645\n",
       "3     0.32924  -4.45520   4.5718 -0.98880\n",
       "4     4.36840   9.67180  -3.9606 -3.16250\n",
       "...       ...       ...      ...      ...\n",
       "1092  0.40614   1.34920  -1.4501 -0.55949\n",
       "1093 -1.38870  -4.87730   6.4774  0.34179\n",
       "1094 -3.75030 -13.45860  17.5932 -2.77710\n",
       "1095 -3.56370  -8.38270  12.3930 -1.28230\n",
       "1096 -2.54190  -0.65804   2.6842  1.19520\n",
       "\n",
       "[1097 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClass = LoadPreprocessData(\"data_banknote_authentication.txt\", None)\n",
    "dataClass.normalize()\n",
    "dataClass.train_test_split()\n",
    "dataClass.train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, reg_factor = 0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "        self.reg_factor = reg_factor\n",
    "    def initialize_weights(self, X, weights = \"0\"):\n",
    "        if weights == \"0\":\n",
    "            self.theta = np.zeros(X.shape[1])\n",
    "        elif weights == \"gaussian\":\n",
    "            self.theta = np.random.randn(X.shape[1])\n",
    "        elif weights == \"uniform\":\n",
    "            self.theta = np.random.uniform(size = X.shape[1])\n",
    "        elif weights == \"xavier\":\n",
    "            self.theta = np.random.randn(X.shape[1]) * np.sqrt(1 / X.shape[1])\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y, type = \"no regularisation\"):\n",
    "        if(type == \"no regularisation\"):\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        \n",
    "        elif type == \"L1\":\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() - 0.5 * self.reg_factor * np.sum(np.abs(self.theta)) / y.size\n",
    "        \n",
    "        else:\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() - 0.5 * self.reg_factor * np.sum(self.theta ** 2) / y.size\n",
    "    \n",
    "    def fit(self, X, y , type = \"L1\", weights = \"0\"):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        self.initialize_weights(X, weights)\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            if type == \"no regularisation\":\n",
    "                gradient = np.dot(X.T, (h - y))\n",
    "                \n",
    "            elif type == \"L1\":\n",
    "                gradient = np.dot(X.T, (h - y)) + 0.5 * self.reg_factor * np.sign(self.theta)\n",
    "                \n",
    "            else:\n",
    "                gradient = np.dot(X.T, (h - y)) + 0.5 * self.reg_factor * self.theta \n",
    "                \n",
    "            self.theta -= self.lr * gradient / y.size\n",
    "            \n",
    "            if(self.verbose == True and i % 5000 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'loss: {self.__loss(h, y, type)} Iteration: {i}\\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5348812181729263 Iteration: 0\t\n",
      "loss: 0.025434507474573684 Iteration: 5000\t\n",
      "loss: 0.02290929366763632 Iteration: 10000\t\n",
      "loss: 0.021857439151567452 Iteration: 15000\t\n",
      "Wall time: 5.11 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.73859538, -4.91054263, -2.62109194, -3.27117166, -0.22696422])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4352976799653367 Iteration: 0\t\n",
      "loss: 0.02542685763807213 Iteration: 5000\t\n",
      "loss: 0.022906757279694656 Iteration: 10000\t\n",
      "loss: 0.02185613518398918 Iteration: 15000\t\n",
      "Wall time: 5.04 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.73904195, -4.91107422, -2.62135814, -3.27151962, -0.22701152])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.510235539857985 Iteration: 0\t\n",
      "loss: 0.02544406161274025 Iteration: 5000\t\n",
      "loss: 0.02291245371002464 Iteration: 10000\t\n",
      "loss: 0.021859062954461057 Iteration: 15000\t\n",
      "Wall time: 5.04 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.73803964, -4.90988108, -2.62076067, -3.27073864, -0.22690537])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.7712402770005047 Iteration: 0\t\n",
      "loss: 0.02534036625794609 Iteration: 5000\t\n",
      "loss: 0.022877847049182942 Iteration: 10000\t\n",
      "loss: 0.0218412326525956 Iteration: 15000\t\n",
      "Wall time: 5.12 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.74416421, -4.91717153, -2.62441156, -3.27551085, -0.2275545 ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6628869831975659 Iteration: 0\t\n",
      "loss: 0.049656686307088964 Iteration: 5000\t\n",
      "loss: 0.03798090977608561 Iteration: 10000\t\n",
      "loss: 0.03343014898820411 Iteration: 15000\t\n",
      "Wall time: 5.04 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.71518456, -2.60527573, -1.46601294, -1.75917724, -0.13022544])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.7552288476969102 Iteration: 0\t\n",
      "loss: 0.04635276380457458 Iteration: 5000\t\n",
      "loss: 0.03695113212824926 Iteration: 10000\t\n",
      "loss: 0.032929502963661574 Iteration: 15000\t\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.75483066, -2.62972526, -1.47661618, -1.77478445, -0.12361187])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.6480945320815197 Iteration: 0\t\n",
      "loss: 0.049036396587628206 Iteration: 5000\t\n",
      "loss: 0.037762760625133646 Iteration: 10000\t\n",
      "loss: 0.03331294473274763 Iteration: 15000\t\n",
      "Wall time: 5.16 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.72054448, -2.61344883, -1.47080603, -1.76502575, -0.13127609])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.5602605679597636 Iteration: 0\t\n",
      "loss: 0.050845274598387974 Iteration: 5000\t\n",
      "loss: 0.038148466089729 Iteration: 10000\t\n",
      "loss: 0.03345815068262117 Iteration: 15000\t\n",
      "Wall time: 5.1 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.70349294, -2.61159051, -1.47221469, -1.76494879, -0.1375662 ])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927272727272727"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6928298003995645 Iteration: 0\t\n",
      "loss: 0.34669132798085256 Iteration: 5000\t\n",
      "loss: 0.26466226321792974 Iteration: 10000\t\n",
      "loss: 0.22424002162157078 Iteration: 15000\t\n",
      "Wall time: 5.09 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9563636363636364"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.103434  , -0.66397523, -0.33266704, -0.29078877, -0.15172144])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9563636363636364"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9500000000000001"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3879237745945316 Iteration: 0\t\n",
      "loss: 0.515464917384462 Iteration: 5000\t\n",
      "loss: 0.2949066850047931 Iteration: 10000\t\n",
      "loss: 0.22188437330343003 Iteration: 15000\t\n",
      "Wall time: 5.16 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9672727272727273"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61178288, -0.73275312, -0.26832165, -0.298004  ,  0.13764107])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9672727272727273"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9632653061224489"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.966397451973528 Iteration: 0\t\n",
      "loss: 0.4792287621876961 Iteration: 5000\t\n",
      "loss: 0.3565550566408354 Iteration: 10000\t\n",
      "loss: 0.30037147485099425 Iteration: 15000\t\n",
      "Wall time: 5.08 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9163636363636364"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.47376856, -0.64098585, -0.27199619, -0.19417951, -0.18902487])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9163636363636364"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986784140969163"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.971856764442229 Iteration: 0\t\n",
      "loss: 0.21595832206408289 Iteration: 5000\t\n",
      "loss: 0.16481537931465784 Iteration: 10000\t\n",
      "loss: 0.140503855230283 Iteration: 15000\t\n",
      "Wall time: 5.1 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9781818181818182"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25079146, -0.8988991 , -0.3838311 , -0.46296851,  0.15844079])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9781818181818182"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
