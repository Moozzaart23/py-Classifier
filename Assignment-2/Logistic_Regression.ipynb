{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadPreprocessData:\n",
    "    def __init__(self, path, header = None):\n",
    "        self.dataset = pd.read_csv(path, header = header)\n",
    "        self.X = self.dataset.iloc[:, :4]\n",
    "        self.y = self.dataset.iloc[:, 4]\n",
    "        \n",
    "    def normalize(self, axis = 0):\n",
    "        return ((self.X - self.X.mean(axis = axis)) / self.X.std(axis = axis))\n",
    "    \n",
    "    def train_test_split(self, percent = 80):\n",
    "        train_rows = random.sample(range(0, self.y.size), percent * self.y.size // 100)\n",
    "        train_rows.sort()\n",
    "        test_rows=[rows for rows in self.X.index.values if rows not in train_rows]\n",
    "        self.train_X = self.X.iloc[train_rows].reset_index(drop = True) \n",
    "        self.train_y = self.y.iloc[train_rows].reset_index(drop = True) \n",
    "        \n",
    "        self.test_X = self.X.iloc[test_rows].reset_index(drop = True) \n",
    "        self.test_y = self.y.iloc[test_rows].reset_index(drop = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.80730</td>\n",
       "      <td>-0.44699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.45860</td>\n",
       "      <td>-1.46210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.92420</td>\n",
       "      <td>0.10645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.01120</td>\n",
       "      <td>-3.59440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.57180</td>\n",
       "      <td>-0.98880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>-2.41000</td>\n",
       "      <td>3.74330</td>\n",
       "      <td>-0.40215</td>\n",
       "      <td>-1.29530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.45010</td>\n",
       "      <td>-0.55949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.59320</td>\n",
       "      <td>-2.77710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.39300</td>\n",
       "      <td>-1.28230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.68420</td>\n",
       "      <td>1.19520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2        3\n",
       "0     3.62160   8.66610  -2.80730 -0.44699\n",
       "1     4.54590   8.16740  -2.45860 -1.46210\n",
       "2     3.86600  -2.63830   1.92420  0.10645\n",
       "3     3.45660   9.52280  -4.01120 -3.59440\n",
       "4     0.32924  -4.45520   4.57180 -0.98880\n",
       "...       ...       ...       ...      ...\n",
       "1092 -2.41000   3.74330  -0.40215 -1.29530\n",
       "1093  0.40614   1.34920  -1.45010 -0.55949\n",
       "1094 -3.75030 -13.45860  17.59320 -2.77710\n",
       "1095 -3.56370  -8.38270  12.39300 -1.28230\n",
       "1096 -2.54190  -0.65804   2.68420  1.19520\n",
       "\n",
       "[1097 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClass = LoadPreprocessData(\"data_banknote_authentication.txt\", None)\n",
    "dataClass.normalize()\n",
    "dataClass.train_test_split()\n",
    "dataClass.train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, reg_factor = 0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "        self.reg_factor = reg_factor\n",
    "    def initialize_weights(self, X, weights = \"0\"):\n",
    "        if weights == \"0\":\n",
    "            self.theta = np.zeros(X.shape[1])\n",
    "        elif weights == \"gaussian\":\n",
    "            self.theta = np.random.randn(X.shape[1])\n",
    "        elif weights == \"uniform\":\n",
    "            self.theta = np.random.uniform(size = X.shape[1])\n",
    "        elif weights == \"xavier\":\n",
    "            self.theta = np.random.randn(X.shape[1]) * np.sqrt(1 / X.shape[1])\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y, type = \"no regularisation\"):\n",
    "        if(type == \"no regularisation\"):\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        \n",
    "        elif type == \"L1\":\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() - 0.5 * self.reg_factor * np.sum(np.abs(self.theta)) / y.size\n",
    "        \n",
    "        else:\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() - 0.5 * self.reg_factor * np.sum(self.theta ** 2) / y.size\n",
    "    \n",
    "    def fit(self, X, y , type = \"no regularisation\", weights = \"0\"):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        self.initialize_weights(X, weights)\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            if type == \"no regularisation\":\n",
    "                gradient = np.dot(X.T, (h - y))\n",
    "                \n",
    "            elif type == \"L1\":\n",
    "                gradient = np.dot(X.T, (h - y)) + 0.5 * self.reg_factor * np.sign(self.theta)\n",
    "                \n",
    "            else:\n",
    "                gradient = np.dot(X.T, (h - y)) + 0.5 * self.reg_factor * self.theta \n",
    "                \n",
    "            self.theta -= self.lr * gradient / y.size\n",
    "            \n",
    "            if(self.verbose == True and i % 5000 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'loss: {self.__loss(h, y, type)} Iteration: {i}\\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9745345089337842 Iteration: 0\t\n",
      "loss: 0.022386822465845134 Iteration: 5000\t\n",
      "loss: 0.020080813339752132 Iteration: 10000\t\n",
      "loss: 0.01912433048377967 Iteration: 15000\t\n",
      "Wall time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979757085020243"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8049980233446538 Iteration: 0\t\n",
      "loss: 0.022437208221555795 Iteration: 5000\t\n",
      "loss: 0.02009725433724085 Iteration: 10000\t\n",
      "loss: 0.019132763613749233 Iteration: 15000\t\n",
      "Wall time: 5 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979757085020243"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5102929335389772 Iteration: 0\t\n",
      "loss: 0.02245830276133439 Iteration: 5000\t\n",
      "loss: 0.02010408943527672 Iteration: 10000\t\n",
      "loss: 0.019136261871582672 Iteration: 15000\t\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979757085020243"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.0639457138876307 Iteration: 0\t\n",
      "loss: 0.022372157732030114 Iteration: 5000\t\n",
      "loss: 0.020075989383641243 Iteration: 10000\t\n",
      "loss: 0.019121851119578898 Iteration: 15000\t\n",
      "Wall time: 5.09 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979757085020243"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6631208142724372 Iteration: 0\t\n",
      "loss: 0.046819128936556036 Iteration: 5000\t\n",
      "loss: 0.03487283920471139 Iteration: 10000\t\n",
      "loss: 0.030261329539015534 Iteration: 15000\t\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.72069982, -2.56213172, -1.54552971, -1.81023256, -0.17765387])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838709677419355"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.002107878247501 Iteration: 0\t\n",
      "loss: 0.04530475711981196 Iteration: 5000\t\n",
      "loss: 0.03442915685179356 Iteration: 10000\t\n",
      "loss: 0.030057404796394824 Iteration: 15000\t\n",
      "Wall time: 5 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.73874193, -2.56977832, -1.54869569, -1.81534201, -0.17389282])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838709677419355"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.4514797226080627 Iteration: 0\t\n",
      "loss: 0.0481342492808312 Iteration: 5000\t\n",
      "loss: 0.03520692850811506 Iteration: 10000\t\n",
      "loss: 0.030400873314857628 Iteration: 15000\t\n",
      "Wall time: 5.13 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.70647755, -2.55885769, -1.54498312, -1.80836148, -0.18174903])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838709677419355"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9289563626039489 Iteration: 0\t\n",
      "loss: 0.04862099335912129 Iteration: 5000\t\n",
      "loss: 0.03533020129578954 Iteration: 10000\t\n",
      "loss: 0.030454739708559785 Iteration: 15000\t\n",
      "Wall time: 5.08 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.70209341, -2.55687753, -1.54411001, -1.80700861, -0.18259277])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838709677419355"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6928324904688002 Iteration: 0\t\n",
      "loss: 0.3443839969945055 Iteration: 5000\t\n",
      "loss: 0.2622539457986278 Iteration: 10000\t\n",
      "loss: 0.22195704487631504 Iteration: 15000\t\n",
      "Wall time: 5.04 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9418181818181818"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9418181818181818"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9327731092436974"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.7126224801747054 Iteration: 0\t\n",
      "loss: 0.7036854148173848 Iteration: 5000\t\n",
      "loss: 0.355410054169544 Iteration: 10000\t\n",
      "loss: 0.2458675665531604 Iteration: 15000\t\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709090909090909"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709090909090909"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.014424372147096 Iteration: 0\t\n",
      "loss: 0.7186252990328104 Iteration: 5000\t\n",
      "loss: 0.36110994200363844 Iteration: 10000\t\n",
      "loss: 0.25524436625836017 Iteration: 15000\t\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9672727272727273"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9672727272727273"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9623430962343097"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.67773829064788 Iteration: 0\t\n",
      "loss: 0.511243525067533 Iteration: 5000\t\n",
      "loss: 0.3201875745329449 Iteration: 10000\t\n",
      "loss: 0.2555407548231718 Iteration: 15000\t\n",
      "Wall time: 4.97 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9309090909090909"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9309090909090909"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9191489361702128"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
