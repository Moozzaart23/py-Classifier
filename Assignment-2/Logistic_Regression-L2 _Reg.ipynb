{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadPreprocessData:\n",
    "    def __init__(self, path, header = None):\n",
    "        self.dataset = pd.read_csv(path, header = header)\n",
    "        self.X = self.dataset.iloc[:, :4]\n",
    "        self.y = self.dataset.iloc[:, 4]\n",
    "        \n",
    "    def normalize(self, axis = 0):\n",
    "        return ((self.X - self.X.mean(axis = axis)) / self.X.std(axis = axis))\n",
    "    \n",
    "    def train_test_split(self, percent = 80):\n",
    "        train_rows = random.sample(range(0, self.y.size), percent * self.y.size // 100)\n",
    "        train_rows.sort()\n",
    "        test_rows=[rows for rows in self.X.index.values if rows not in train_rows]\n",
    "        self.train_X = self.X.iloc[train_rows].reset_index(drop = True) \n",
    "        self.train_y = self.y.iloc[train_rows].reset_index(drop = True) \n",
    "        \n",
    "        self.test_X = self.X.iloc[test_rows].reset_index(drop = True) \n",
    "        self.test_y = self.y.iloc[test_rows].reset_index(drop = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1        2        3\n",
       "0     3.62160   8.66610  -2.8073 -0.44699\n",
       "1     4.54590   8.16740  -2.4586 -1.46210\n",
       "2     3.86600  -2.63830   1.9242  0.10645\n",
       "3     3.45660   9.52280  -4.0112 -3.59440\n",
       "4     0.32924  -4.45520   4.5718 -0.98880\n",
       "...       ...       ...      ...      ...\n",
       "1092  0.40614   1.34920  -1.4501 -0.55949\n",
       "1093 -1.38870  -4.87730   6.4774  0.34179\n",
       "1094 -3.75030 -13.45860  17.5932 -2.77710\n",
       "1095 -3.56370  -8.38270  12.3930 -1.28230\n",
       "1096 -2.54190  -0.65804   2.6842  1.19520\n",
       "\n",
       "[1097 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClass = LoadPreprocessData(\"data_banknote_authentication.txt\", None)\n",
    "dataClass.normalize()\n",
    "dataClass.train_test_split()\n",
    "dataClass.train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, reg_factor = 0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "        self.reg_factor = reg_factor\n",
    "    def initialize_weights(self, X, weights = \"0\"):\n",
    "        if weights == \"0\":\n",
    "            self.theta = np.zeros(X.shape[1])\n",
    "        elif weights == \"gaussian\":\n",
    "            self.theta = np.random.randn(X.shape[1])\n",
    "        elif weights == \"uniform\":\n",
    "            self.theta = np.random.uniform(size = X.shape[1])\n",
    "        elif weights == \"xavier\":\n",
    "            self.theta = np.random.randn(X.shape[1]) * np.sqrt(1 / X.shape[1])\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y, type = \"no regularisation\"):\n",
    "        if(type == \"no regularisation\"):\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        \n",
    "        elif type == \"L1\":\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() - 0.5 * self.reg_factor * np.sum(np.abs(self.theta)) / y.size\n",
    "        \n",
    "        else:\n",
    "            return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() - 0.5 * self.reg_factor * np.sum(self.theta ** 2) / y.size\n",
    "    \n",
    "    def fit(self, X, y , type = \"L2\", weights = \"0\"):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        self.initialize_weights(X, weights)\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            if type == \"no regularisation\":\n",
    "                gradient = np.dot(X.T, (h - y))\n",
    "                \n",
    "            elif type == \"L1\":\n",
    "                gradient = np.dot(X.T, (h - y)) + 0.5 * self.reg_factor * np.sign(self.theta)\n",
    "                \n",
    "            else:\n",
    "                gradient = np.dot(X.T, (h - y)) + 0.5 * self.reg_factor * self.theta \n",
    "                \n",
    "            self.theta -= self.lr * gradient / y.size\n",
    "            \n",
    "            if(self.verbose == True and i % 5000 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'loss: {self.__loss(h, y, type)} Iteration: {i}\\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.675200920653825 Iteration: 0\t\n",
      "loss: 0.02424214398614986 Iteration: 5000\t\n",
      "loss: 0.021438715285342105 Iteration: 10000\t\n",
      "loss: 0.020172786091272415 Iteration: 15000\t\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.73449056, -5.11265963, -2.74830395, -3.39997075, -0.2581917 ])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983050847457627"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.27736067065855097 Iteration: 0\t\n",
      "loss: 0.02423264072309868 Iteration: 5000\t\n",
      "loss: 0.0214352735045491 Iteration: 10000\t\n",
      "loss: 0.020170891126263035 Iteration: 15000\t\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.73503851, -5.11335792, -2.74865104, -3.40042049, -0.25825942])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983050847457627"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.509406850758869 Iteration: 0\t\n",
      "loss: 0.024298635586253437 Iteration: 5000\t\n",
      "loss: 0.021459006815230403 Iteration: 10000\t\n",
      "loss: 0.020183939577149163 Iteration: 15000\t\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.7312718 , -5.10855791, -2.74626515, -3.39732902, -0.25779409])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983050847457627"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.549175298723947 Iteration: 0\t\n",
      "loss: 0.02433049451771495 Iteration: 5000\t\n",
      "loss: 0.021470342949200284 Iteration: 10000\t\n",
      "loss: 0.02019015695694993 Iteration: 15000\t\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.72948235, -5.10627758, -2.74513171, -3.39586037, -0.25757312])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983050847457627"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6628124020031013 Iteration: 0\t\n",
      "loss: 0.04924654705850929 Iteration: 5000\t\n",
      "loss: 0.03732766948559046 Iteration: 10000\t\n",
      "loss: 0.03266024046983874 Iteration: 15000\t\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.69616006, -2.64669282, -1.50026401, -1.78476714, -0.10017378])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983050847457627"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.085047315419675 Iteration: 0\t\n",
      "loss: 0.04968760838235737 Iteration: 5000\t\n",
      "loss: 0.037458622104509724 Iteration: 10000\t\n",
      "loss: 0.03272421722859906 Iteration: 15000\t\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.69207466, -2.64326713, -1.498501  , -1.78243257, -0.10047021])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983050847457627"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2679312664296707 Iteration: 0\t\n",
      "loss: 0.04528540802130191 Iteration: 5000\t\n",
      "loss: 0.03604989721798137 Iteration: 10000\t\n",
      "loss: 0.03201897161086677 Iteration: 15000\t\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.74045543, -2.68149747, -1.51764326, -1.80816848, -0.09585506])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983050847457627"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.21331740317697462 Iteration: 0\t\n",
      "loss: 0.04116705429617884 Iteration: 5000\t\n",
      "loss: 0.03446788588938644 Iteration: 10000\t\n",
      "loss: 0.031156000953232784 Iteration: 15000\t\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.80102662, -2.73559621, -1.54568004, -1.8449781 , -0.09262387])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9854545454545455"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983050847457627"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6928290524614211 Iteration: 0\t\n",
      "loss: 0.3458834252299731 Iteration: 5000\t\n",
      "loss: 0.2640747576374346 Iteration: 10000\t\n",
      "loss: 0.223854443663995 Iteration: 15000\t\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9418181818181818"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10156493, -0.67224361, -0.33153082, -0.27966781, -0.13711002])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9418181818181818"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4677734903104547 Iteration: 0\t\n",
      "loss: 0.9567065628156952 Iteration: 5000\t\n",
      "loss: 0.40553399961455855 Iteration: 10000\t\n",
      "loss: 0.26310648210042603 Iteration: 15000\t\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9272727272727272"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52407638, -0.66689915, -0.23574021, -0.23762148,  0.14702761])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9272727272727272"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.690305098055024 Iteration: 0\t\n",
      "loss: 0.7194031125123025 Iteration: 5000\t\n",
      "loss: 0.36247927277108816 Iteration: 10000\t\n",
      "loss: 0.26822901181092973 Iteration: 15000\t\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9490909090909091"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00182642, -0.56190925, -0.3220027 , -0.24925233, -0.17605471])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9490909090909091"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9380530973451328"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.0833848660155394 Iteration: 0\t\n",
      "loss: 0.714689082948707 Iteration: 5000\t\n",
      "loss: 0.3195781201611715 Iteration: 10000\t\n",
      "loss: 0.22787683794116478 Iteration: 15000\t\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.0001, num_iter=20000, verbose = True)\n",
    "%time model.fit(dataClass.train_X, dataClass.train_y, weights = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "# accuracy\n",
    "(preds == dataClass.test_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53274848, -0.56993014, -0.3754901 , -0.32948634, -0.15240939])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.theta\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(dataClass.test_X, threshold = 0.5)\n",
    "actual = dataClass.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "    def accuracy(self):\n",
    "        return (self.predicted == self.actual).mean()\n",
    "    def F_score(self):\n",
    "        counter = Counter(zip(self.predicted, self.actual))\n",
    "        self.truePositive = counter[True, 1]\n",
    "        self.falsePositive = counter[True, 0]\n",
    "        self.trueNegative = counter[False, 0]\n",
    "        self.falseNegative = counter[False, 1]\n",
    "        \n",
    "        self.precision = self.truePositive / (self.truePositive + self.falsePositive)\n",
    "        self.recall = self.truePositive / (self.truePositive + self.falseNegative)\n",
    "        self.fScore = (2 * self.precision * self.recall) / (self.precision + self.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181818181818"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = Accuracy(predicted, actual)\n",
    "accuracy.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789029535864979"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.F_score()\n",
    "accuracy.fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
